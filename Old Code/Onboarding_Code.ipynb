{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FACILITY \n",
    "\n",
    "This is the Facility Data that will import all the mandatory columns in the (Asset-Facility Template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7876\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7876/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\remen\\AppData\\Local\\Temp\\ipykernel_8784\\200874018.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[required_columns[\"criticality\"]] = df[required_columns[\"criticality\"]].str.extract(r\"(C\\d)\")\n",
      "C:\\Users\\remen\\AppData\\Local\\Temp\\ipykernel_8784\\200874018.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[required_columns[\"location.longitude\"]] = pd.to_numeric(df[required_columns[\"location.longitude\"]], errors=\"coerce\")\n",
      "C:\\Users\\remen\\AppData\\Local\\Temp\\ipykernel_8784\\200874018.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[required_columns[\"location.latitude\"]] = pd.to_numeric(df[required_columns[\"location.latitude\"]], errors=\"coerce\")\n",
      "C:\\Users\\remen\\AppData\\Local\\Temp\\ipykernel_8784\\200874018.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"location.country\"] = df[required_columns[\"location.location\"]].str.split(\",\").str[-1].str.strip()\n",
      "C:\\Users\\remen\\AppData\\Local\\Temp\\ipykernel_8784\\200874018.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"location.city\"] = df[required_columns[\"location.location\"]].str.split(\",\").str[-2].str.strip()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gradio as gr\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Load column mappings from a JSON file for flexibility\n",
    "def load_column_mappings(config_file=\"column_mappings.json\"):\n",
    "    try:\n",
    "        with open(config_file, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        return {\n",
    "            \"name*\": \"Building Name\",\n",
    "            \"facilityType*\": \"Facility Type\",\n",
    "            \"criticality\": \"Building Criticality\",\n",
    "            \"location.longitude\": \"Longitude\",\n",
    "            \"location.latitude\": \"Latitude\",\n",
    "            \"location.location\": \"Building Address\",\n",
    "        }\n",
    "\n",
    "# Normalize column headers\n",
    "def normalize_headers(df):\n",
    "    df.columns = df.columns.str.strip()  # Remove leading/trailing spaces\n",
    "    df.columns = df.columns.str.replace(r\"\\s+\", \" \", regex=True)  # Replace multiple spaces with one\n",
    "    return df\n",
    "\n",
    "# Function to validate and clean data\n",
    "def validate_and_clean_data(df, required_columns, valid_criticality_values):\n",
    "    # Normalize column headers\n",
    "    df = normalize_headers(df)\n",
    "\n",
    "    # Remove rows that are placeholders (e.g., \"Mandatory\")\n",
    "    df = df[~df[required_columns[\"location.location\"]].str.contains(\"Mandatory\", case=False, na=False)]\n",
    "\n",
    "    # Normalize criticality\n",
    "    if required_columns[\"criticality\"] in df:\n",
    "        df[required_columns[\"criticality\"]] = df[required_columns[\"criticality\"]].str.extract(r\"(C\\d)\")\n",
    "\n",
    "    # Convert longitude and latitude to numeric\n",
    "    if required_columns[\"location.longitude\"] in df and required_columns[\"location.latitude\"] in df:\n",
    "        df[required_columns[\"location.longitude\"]] = pd.to_numeric(df[required_columns[\"location.longitude\"]], errors=\"coerce\")\n",
    "        df[required_columns[\"location.latitude\"]] = pd.to_numeric(df[required_columns[\"location.latitude\"]], errors=\"coerce\")\n",
    "\n",
    "    # Extract country and city from the address column\n",
    "    if required_columns[\"location.location\"] in df:\n",
    "        # Split address into country and city\n",
    "        df[\"location.country\"] = df[required_columns[\"location.location\"]].str.split(\",\").str[-1].str.strip()\n",
    "        df[\"location.city\"] = df[required_columns[\"location.location\"]].str.split(\",\").str[-2].str.strip()\n",
    "\n",
    "    # Filter valid rows\n",
    "    df = df[\n",
    "        df[required_columns[\"name*\"]].notna() &\n",
    "        (~df[required_columns[\"name*\"]].str.contains(\"Mandatory|name\", case=False, na=False)) &\n",
    "        (~df[required_columns[\"facilityType*\"]].str.contains(\"Mandatory|facility type\", case=False, na=False)) &\n",
    "        (df[required_columns[\"criticality\"]].isin(valid_criticality_values))\n",
    "    ]\n",
    "\n",
    "    return df\n",
    "\n",
    "# Main processing function\n",
    "def process_data(afm_file, facility_template_file, namespace, output_path):\n",
    "    try:\n",
    "        # Load column mappings\n",
    "        column_mappings = load_column_mappings()\n",
    "\n",
    "        # Define valid criticality values\n",
    "        valid_criticality_values = [\"C1\", \"C2\", \"C3\"]\n",
    "\n",
    "        # Load the AFM data\n",
    "        afm_data = pd.read_excel(afm_file.name, sheet_name=\"Building (Facility)\", engine=\"openpyxl\")\n",
    "\n",
    "        # Normalize column headers\n",
    "        afm_data = normalize_headers(afm_data)\n",
    "\n",
    "        # Load the facility template\n",
    "        facility_template_data = pd.read_csv(facility_template_file.name)\n",
    "\n",
    "        # Validate and clean AFM data\n",
    "        cleaned_afm_data = validate_and_clean_data(afm_data, column_mappings, valid_criticality_values)\n",
    "\n",
    "        # Create new facility data\n",
    "        facility_data = pd.DataFrame({\n",
    "            \"name*\": cleaned_afm_data[column_mappings[\"name*\"]],\n",
    "            \"facilityType*\": cleaned_afm_data[column_mappings[\"facilityType*\"]],\n",
    "            \"criticality\": cleaned_afm_data[column_mappings[\"criticality\"]],\n",
    "            \"location.longitude\": cleaned_afm_data[column_mappings[\"location.longitude\"]],\n",
    "            \"location.latitude\": cleaned_afm_data[column_mappings[\"location.latitude\"]],\n",
    "            \"location.location\": cleaned_afm_data[column_mappings[\"location.location\"]],\n",
    "            \"location.country\": cleaned_afm_data[\"location.country\"],\n",
    "            \"location.city\": cleaned_afm_data[\"location.city\"],\n",
    "            \"isActive*\": True,\n",
    "            \"namespace*\": namespace,\n",
    "        })\n",
    "\n",
    "        # Remove placeholder rows from the template\n",
    "        facility_template_data = facility_template_data.drop(index=[0, 1], errors=\"ignore\").reset_index(drop=True)\n",
    "\n",
    "        # Merge the cleaned facility data with the template\n",
    "        updated_facility_template = pd.concat([facility_template_data, facility_data], ignore_index=True)\n",
    "\n",
    "        # Ensure output directory exists\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "        # Save the updated template\n",
    "        updated_facility_template.to_csv(output_path, index=False)\n",
    "\n",
    "        return f\"Data processed successfully! Updated file saved at: {output_path}\"\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred during processing: {str(e)}\"\n",
    "\n",
    "# Create the Gradio interface\n",
    "interface = gr.Interface(\n",
    "    fn=process_data,\n",
    "    inputs=[\n",
    "        gr.File(label=\"Upload Customer Template (.xlsx)\"),\n",
    "        gr.File(label=\"Upload Facility Template (.csv)\"),\n",
    "        gr.Textbox(label=\"Namespace*\", placeholder=\"Enter namespace here\"),\n",
    "        gr.Textbox(label=\"Output Path\", placeholder=\"Enter the full output file path\"),\n",
    "    ],\n",
    "    outputs=\"text\",\n",
    "    title=\"FACILITY DATA ONBOARDING\",\n",
    "    description=(\n",
    "        \"Upload your AFM and Facility Template files, enter the namespace, specify the output file path, \"\n",
    "        \"and process the data. The updated file will include separated country and city columns.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "interface.launch()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After Importing (Asset-Facility Template) to system you will get ID's for Facilities which will be imported to (Asset-Location Template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Location "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7877\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7877/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\remen\\AppData\\Local\\Temp\\ipykernel_8784\\555640678.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valid_data['Floor_normalized'] = valid_data['Floor'].str.replace(\" \", \"\").str.replace(\"- \", \"-\")\n",
      "C:\\Users\\remen\\AppData\\Local\\Temp\\ipykernel_8784\\555640678.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valid_data['Floor_normalized'] = valid_data['Floor_normalized'].str.replace(\"Core\", \"Core \", regex=False).str.replace(\"LVL\", \"LVL \", regex=False)\n",
      "C:\\Users\\remen\\AppData\\Local\\Temp\\ipykernel_8784\\555640678.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valid_data['Criticality'] = valid_data['Floor Criticality'].str.extract(r\"(C\\d)\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gradio as gr\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "# Define the function for data processing\n",
    "def process_asset_location_data(asset_location_file, csv_file, output_path, namespace):\n",
    "    try:\n",
    "        # Validate that namespace is provided\n",
    "        if not namespace.strip():\n",
    "            return \"Error: Namespace* must be provided.\"\n",
    "\n",
    "        # Load the input files\n",
    "        asset_location_data = pd.read_excel(asset_location_file.name, sheet_name='Asset,location')\n",
    "        csv_data = pd.read_csv(csv_file.name)\n",
    "\n",
    "        # Step 1: Clean the CSV data by removing the first row and placeholder values\n",
    "        csv_data_cleaned = csv_data.iloc[1:]\n",
    "\n",
    "        # Step 2: Extract unique 'Building', 'Floor', and 'Floor Criticality' data\n",
    "        unique_building_floor_criticality = asset_location_data[['Building', 'Floor', 'Floor Criticality']].drop_duplicates().dropna()\n",
    "\n",
    "        # Step 3: Filter out rows with placeholder values like 'Mandatory' or NaN\n",
    "        valid_data = unique_building_floor_criticality[\n",
    "            (unique_building_floor_criticality['Building'] != 'Mandatory') & \n",
    "            (unique_building_floor_criticality['Building'].notna()) & \n",
    "            (unique_building_floor_criticality['Floor'] != 'Mandatory') & \n",
    "            (unique_building_floor_criticality['Floor'].notna())\n",
    "        ]\n",
    "\n",
    "        # Step 4: Normalize floor values by removing extra spaces and standardizing formatting\n",
    "        valid_data['Floor_normalized'] = valid_data['Floor'].str.replace(\" \", \"\").str.replace(\"- \", \"-\")\n",
    "        valid_data['Floor_normalized'] = valid_data['Floor_normalized'].str.replace(\"Core\", \"Core \", regex=False).str.replace(\"LVL\", \"LVL \", regex=False)\n",
    "\n",
    "        # Step 5: Normalize and validate criticality values\n",
    "        # Extract only valid criticality values (C1, C2, C3) using regex\n",
    "        valid_data['Criticality'] = valid_data['Floor Criticality'].str.extract(r\"(C\\d)\")\n",
    "        \n",
    "        # Define valid criticality values\n",
    "        valid_criticality_values = [\"C1\", \"C2\", \"C3\"]\n",
    "        \n",
    "        # Filter rows based on valid criticality values\n",
    "        valid_data = valid_data[valid_data['Criticality'].isin(valid_criticality_values)]\n",
    "\n",
    "        # Step 6: Create a new DataFrame with cleaned building, normalized floor, and criticality data\n",
    "        facility_name_column_cleaned = valid_data['Building'].reset_index(drop=True)\n",
    "        name_column_cleaned = valid_data['Floor_normalized'].reset_index(drop=True)\n",
    "        criticality_column_cleaned = valid_data['Criticality'].reset_index(drop=True)\n",
    "\n",
    "        # Fill the `namespace*` column with the provided namespace value\n",
    "        new_cleaned_data = pd.DataFrame({\n",
    "            'facility*': [None] * len(facility_name_column_cleaned),\n",
    "            'facility name': facility_name_column_cleaned,\n",
    "            'name*': name_column_cleaned,\n",
    "            'criticality': criticality_column_cleaned,\n",
    "            'namespace*': [namespace] * len(facility_name_column_cleaned),\n",
    "            'isActive*': [True] * len(facility_name_column_cleaned)  # Assuming active by default\n",
    "        })\n",
    "\n",
    "        # Step 7: Merge the new data with the original CSV template\n",
    "        # Ensure only necessary columns are updated while keeping others as-is\n",
    "        updated_csv_cleaned_data = csv_data_cleaned.copy()\n",
    "\n",
    "        # Add or update the relevant columns in the existing template\n",
    "        for col in ['facility*', 'facility name', 'name*', 'criticality', 'namespace*', 'isActive*']:\n",
    "            if col in new_cleaned_data.columns:\n",
    "                updated_csv_cleaned_data[col] = new_cleaned_data[col]\n",
    "\n",
    "        # Save the cleaned data to the specified output file path\n",
    "        updated_csv_cleaned_data.to_csv(output_path, index=False)\n",
    "\n",
    "        return f\"Data processed successfully! Updated file saved at: {output_path}\"\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred during processing: {str(e)}\"\n",
    "\n",
    "# Create a Gradio interface\n",
    "interface = gr.Interface(\n",
    "    fn=process_asset_location_data,\n",
    "    inputs=[\n",
    "        gr.File(label=\"Upload Asset Location File (.xlsx)\"),\n",
    "        gr.File(label=\"Upload CSV File (.csv)\"),\n",
    "        gr.Textbox(label=\"Output File Path\", placeholder=\"Enter namespace here\"),\n",
    "        gr.Textbox(label=\"Namespace*\", placeholder=\"Enter the full output file path\"),\n",
    "    ],\n",
    "    outputs=\"text\",\n",
    "    title=\"LOCATION DATA ONBOARDING\",\n",
    "    description=(\n",
    "        \"Upload your Asset Location and Facility Template files, specify the namespace, and process the data. \"\n",
    "        \"Criticality values (C1, C2, C3) will be validated and included in the output.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "interface.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This below code to map Facility id with location names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7878\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7878/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gradio as gr\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "# Function to clean and truncate facility names automatically\n",
    "def clean_and_truncate_facility_name(name, substrings_to_ignore):\n",
    "    if isinstance(name, str):\n",
    "        # Remove all unwanted substrings dynamically\n",
    "        for substring in substrings_to_ignore:\n",
    "            name = name.replace(substring, \"\")\n",
    "        # Truncate to the first comma\n",
    "        return name.split(\",\")[0].strip()\n",
    "    return name\n",
    "\n",
    "# Define the Gradio function\n",
    "def process_facility_and_space_data(location_file, space_file, output_file_path, substrings_to_ignore):\n",
    "    try:\n",
    "        # Convert substrings to ignore from string to list\n",
    "        substrings_to_ignore = substrings_to_ignore.split(',')\n",
    "\n",
    "        # Load both files\n",
    "        location_data = pd.read_csv(location_file.name)\n",
    "        space_data = pd.read_csv(space_file.name)\n",
    "\n",
    "        # Clean and truncate the 'name*' column in the location file for facility names\n",
    "        location_data['name_cleaned'] = location_data['name*'].apply(\n",
    "            lambda x: clean_and_truncate_facility_name(x, substrings_to_ignore)\n",
    "        )\n",
    "\n",
    "        # Clean and truncate the 'facility name' column in the space file\n",
    "        space_data['facility_cleaned'] = space_data['facility name'].apply(\n",
    "            lambda x: clean_and_truncate_facility_name(x, substrings_to_ignore)\n",
    "        )\n",
    "\n",
    "        # Create a mapping dictionary from the cleaned location file\n",
    "        facility_mapping = location_data.set_index('name_cleaned')['id'].to_dict()\n",
    "\n",
    "        # Populate the 'facility*' column in the space file based on the mapping\n",
    "        space_data['facility*'] = space_data['facility_cleaned'].map(facility_mapping)\n",
    "\n",
    "        # Drop temporary columns\n",
    "        space_data = space_data.drop(columns=['facility_cleaned'])\n",
    "        location_data = location_data.drop(columns=['name_cleaned'])\n",
    "\n",
    "        # Save the updated space file\n",
    "        space_data.to_csv(output_file_path, index=False)\n",
    "\n",
    "        return f\"Updated file saved at: {output_file_path}\"\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "# Create a Gradio interface\n",
    "interface = gr.Interface(\n",
    "    fn=process_facility_and_space_data,\n",
    "    inputs=[\n",
    "        gr.File(label=\"Upload Exported Facility File (CSV)\"),\n",
    "        gr.File(label=\"Upload Location Template File (CSV)\"),\n",
    "        gr.Textbox(label=\"Output File Path\", placeholder=\"Enter the full output file path\"),\n",
    "        gr.Textbox(\n",
    "            label=\"Substrings to Ignore\",\n",
    "            placeholder=\"Enter substrings to ignore, separated by commas (e.g., Abu Dhabi,Al Ain,Alain)\"\n",
    "        ),\n",
    "    ],\n",
    "    outputs=\"text\",\n",
    "    title=\"EXPORTED FACILITY ID MAPPING IN LOCATION TEMPLATE\",\n",
    "    description=(\n",
    "        \"Upload your location and space CSV files, specify substrings to ignore, and the output file path. \"\n",
    "        \"The tool will clean and truncate facility names and map IDs to save the updated CSV.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPACE CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7879\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7879/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\remen\\AppData\\Local\\Temp\\ipykernel_8784\\932199211.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valid_data['Sublocation_normalized'] = valid_data['Sublocation'].str.strip()\n",
      "C:\\Users\\remen\\AppData\\Local\\Temp\\ipykernel_8784\\932199211.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valid_data['Sublocation_normalized'] = valid_data['Sublocation_normalized'].str.replace(\"  \", \" \").str.replace(\"-\", \" - \")\n",
      "C:\\Users\\remen\\AppData\\Local\\Temp\\ipykernel_8784\\932199211.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valid_data['Criticality'] = valid_data['Floor Criticality'].str.extract(r\"(C\\d)\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import gradio as gr\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "# Define a function to process the uploaded files\n",
    "def process_asset_data(asset_location_file, csv_file, namespace, output_path):\n",
    "    try:\n",
    "        # Load data\n",
    "        asset_location_data = pd.read_excel(asset_location_file.name, sheet_name='Asset,location', engine='openpyxl')\n",
    "        csv_data = pd.read_csv(csv_file.name)\n",
    "\n",
    "        # Step 1: Clean the CSV data by removing the first row and placeholder values\n",
    "        csv_data_cleaned = csv_data.iloc[1:]\n",
    "\n",
    "        # Step 2: Extract unique 'Building', 'Floor', 'Sublocation', and 'Criticality' data\n",
    "        unique_building_floor = asset_location_data[['Building', 'Floor', 'Sublocation', 'Floor Criticality']].drop_duplicates().dropna()\n",
    "\n",
    "        # Step 3: Filter out rows with placeholder values like 'Mandatory' or NaN\n",
    "        valid_data = unique_building_floor[\n",
    "            (unique_building_floor['Building'] != 'Mandatory') &\n",
    "            (unique_building_floor['Building'].notna()) &\n",
    "            (unique_building_floor['Floor'] != 'Mandatory') &\n",
    "            (unique_building_floor['Floor'].notna()) &\n",
    "            (unique_building_floor['Sublocation'] != 'Mandatory') &\n",
    "            (unique_building_floor['Sublocation'].notna()) &\n",
    "            (unique_building_floor['Floor Criticality'].notna())\n",
    "        ]\n",
    "\n",
    "        # Step 4: Normalize the `Sublocation` values (remove extra spaces and standardize formatting)\n",
    "        valid_data['Sublocation_normalized'] = valid_data['Sublocation'].str.strip()\n",
    "        valid_data['Sublocation_normalized'] = valid_data['Sublocation_normalized'].str.replace(\"  \", \" \").str.replace(\"-\", \" - \")\n",
    "\n",
    "        # Step 5: Normalize and validate criticality values\n",
    "        # Extract valid criticality values (C1, C2, C3) using regex\n",
    "        valid_data['Criticality'] = valid_data['Floor Criticality'].str.extract(r\"(C\\d)\")\n",
    "        \n",
    "        # Define valid criticality values\n",
    "        valid_criticality_values = [\"C1\", \"C2\", \"C3\"]\n",
    "        \n",
    "        # Filter rows based on valid criticality values\n",
    "        valid_data = valid_data[valid_data['Criticality'].isin(valid_criticality_values)]\n",
    "\n",
    "        # Extract unique normalized values\n",
    "        valid_data = valid_data.drop_duplicates(subset=['Sublocation_normalized'])\n",
    "\n",
    "        # Step 6: Create a new DataFrame with cleaned building, floor, sublocation, and criticality data\n",
    "        facility_name_column_cleaned = valid_data['Building'].reset_index(drop=True)\n",
    "        location_name_column_cleaned = valid_data['Floor'].reset_index(drop=True)\n",
    "        name_column_cleaned = valid_data['Sublocation_normalized'].reset_index(drop=True)\n",
    "        criticality_column_cleaned = valid_data['Criticality'].reset_index(drop=True)\n",
    "\n",
    "        # Fill the `namespace*` column with the user-provided namespace\n",
    "        new_cleaned_data = pd.DataFrame({\n",
    "            'facility name': facility_name_column_cleaned,\n",
    "            'location name': location_name_column_cleaned,\n",
    "            'name*': name_column_cleaned,\n",
    "            'criticality': criticality_column_cleaned,\n",
    "            'namespace*': [namespace] * len(facility_name_column_cleaned),\n",
    "            'isActive*': [True] * len(facility_name_column_cleaned)  # Assuming active by default\n",
    "        })\n",
    "\n",
    "        # Step 7: Concatenate the cleaned new data with the original cleaned CSV data\n",
    "        updated_csv_cleaned_data = pd.concat([csv_data_cleaned, new_cleaned_data], ignore_index=True)\n",
    "\n",
    "        # Save the cleaned data to the specified output path\n",
    "        updated_csv_cleaned_data.to_csv(output_path, index=False)\n",
    "\n",
    "        return f\"Cleaned data has been successfully saved to: {output_path}\"\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "# Create a Gradio interface\n",
    "interface = gr.Interface(\n",
    "    fn=process_asset_data,\n",
    "    inputs=[\n",
    "        gr.File(label=\"Upload Asset Location File (.xlsx)\"),\n",
    "        gr.File(label=\"Upload Space Template File (.csv)\"),\n",
    "        gr.Textbox(label=\"Namespace*\", placeholder=\"Enter namespace here\"),\n",
    "        gr.Textbox(label=\"Output Path\", placeholder=\"Enter the full output file path\"),\n",
    "    ],\n",
    "    outputs=\"text\",\n",
    "    title=\"SPACE DATA ONBOARDING\",\n",
    "    description=(\n",
    "        \"Upload your Asset Location and CSV files, enter the namespace, specify the output file path, \"\n",
    "        \"and process the data. Criticality values (C1, C2, C3) will be validated and included in the output.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This below code to map Location id with Space Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7880\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7880/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gradio as gr\n",
    "\n",
    "# Function to clean and normalize facility names\n",
    "def clean_and_normalize_facility_name(name):\n",
    "    if isinstance(name, str):\n",
    "        # Normalize spaces around dashes and remove leading/trailing spaces\n",
    "        name = name.replace(\"- \", \"-\").replace(\" -\", \"-\").strip()\n",
    "        # Truncate to the first comma\n",
    "        return name.split(\",\")[0].strip()\n",
    "    return name\n",
    "\n",
    "# Function to process the data\n",
    "def process_facility_data(location_file, space_file, output_path):\n",
    "    try:\n",
    "        # Load the uploaded files\n",
    "        location_data = pd.read_csv(location_file.name)\n",
    "        space_data = pd.read_csv(space_file.name)\n",
    "\n",
    "        # Clean and normalize facility names in the location dataset\n",
    "        location_data['name_cleaned'] = location_data['name*'].apply(clean_and_normalize_facility_name)\n",
    "\n",
    "        # Clean and normalize location names in the space dataset\n",
    "        space_data_temp = space_data.copy()\n",
    "        space_data_temp['location_cleaned'] = space_data_temp['location name'].astype(str).apply(clean_and_normalize_facility_name)\n",
    "\n",
    "        # Create a mapping dictionary using the cleaned location file\n",
    "        if 'facility name' not in location_data.columns or 'name_cleaned' not in location_data.columns or 'id' not in location_data.columns:\n",
    "            return \"Error: Missing required columns ('facility name', 'name*', or 'id') in the location file.\"\n",
    "        \n",
    "        location_mapping = location_data.set_index(['facility name', 'name_cleaned'])['id'].to_dict()\n",
    "\n",
    "        # Populate the \"location*\" column in the space file using the mapping\n",
    "        if 'facility name' not in space_data.columns or 'location name' not in space_data.columns:\n",
    "            return \"Error: Missing required columns ('facility name' or 'location name') in the space file.\"\n",
    "\n",
    "        space_data['location*'] = space_data_temp.apply(\n",
    "            lambda row: location_mapping.get((row['facility name'], row['location_cleaned']), None),\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        # Save the updated space file to the specified output path\n",
    "        if not output_path.strip():\n",
    "            return \"Error: Output path cannot be empty.\"\n",
    "        \n",
    "        space_data.to_csv(output_path, index=False)\n",
    "\n",
    "        return f\"Data processed successfully! Updated file saved at: {output_path}\"\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "# Create a Gradio interface\n",
    "interface = gr.Interface(\n",
    "    fn=process_facility_data,\n",
    "    inputs=[\n",
    "        gr.File(label=\"Upload EXPORTED LOCATION FILE (.csv)\"),\n",
    "        gr.File(label=\"Upload Space File (.csv)\"),\n",
    "        gr.Textbox(label=\"Output Path\", placeholder=\"Enter the full output file path\"),\n",
    "    ],\n",
    "    outputs=\"text\",\n",
    "    title=\"LOCATION ID MAPPING WITH SPACE TEMPLATE\",\n",
    "    description=(\n",
    "        \"Upload your Location and Space CSV files, specify the output file path, \"\n",
    "        \"and process the data. The updated file will overwrite the specified Space file.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "interface.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EQUIPMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7865\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\remen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gradio as gr\n",
    "\n",
    "# Function to process the data\n",
    "def process_asset_equipment_data(asset_location_file, csv_file, namespace, output_path):\n",
    "    try:\n",
    "        # Load data from uploaded files\n",
    "        asset_location_data = pd.read_excel(asset_location_file.name, sheet_name='Asset,location (2)', engine='openpyxl')\n",
    "        csv_data = pd.read_csv(csv_file.name)\n",
    "\n",
    "        # Step 1: Clean the CSV data by removing the first row\n",
    "        csv_data_cleaned = csv_data.iloc[1:]\n",
    "\n",
    "        # Step 2: Extract unique data from the \"Asset,location\" sheet\n",
    "        unique_asset_data = asset_location_data[\n",
    "            ['Barcode', 'Asset System', 'Asset / Equipment', 'Asset Criticality', 'Sublocation']\n",
    "        ].drop_duplicates()\n",
    "\n",
    "        # Step 3: Normalize criticality values\n",
    "        unique_asset_data['Asset Criticality'] = unique_asset_data['Asset Criticality'].str.extract(r'^(C[1-3])')\n",
    "\n",
    "        # Step 4: Filter valid rows\n",
    "        placeholder_values = ['Mandatory', None, '']\n",
    "        valid_data_normalized = unique_asset_data[\n",
    "            (~unique_asset_data['Barcode'].isin(placeholder_values)) &\n",
    "            (~unique_asset_data['Asset System'].isin(placeholder_values)) &\n",
    "            (~unique_asset_data['Asset / Equipment'].isin(placeholder_values)) &\n",
    "            (~unique_asset_data['Sublocation'].isin(placeholder_values)) &\n",
    "            (unique_asset_data['Asset Criticality'].isin(['C1', 'C2', 'C3']))\n",
    "        ]\n",
    "\n",
    "        # Step 5: Populate required columns\n",
    "        barcode_column = valid_data_normalized['Barcode'].reset_index(drop=True)\n",
    "        class_column = valid_data_normalized['Asset System'].reset_index(drop=True)\n",
    "        equipment_type_column = valid_data_normalized['Asset / Equipment'].reset_index(drop=True)\n",
    "        criticality_column = valid_data_normalized['Asset Criticality'].reset_index(drop=True)\n",
    "        asset_name_column = valid_data_normalized['Sublocation'].reset_index(drop=True)\n",
    "\n",
    "        # Fill namespace and other template columns\n",
    "        new_cleaned_data_normalized = pd.DataFrame({\n",
    "            'name*': barcode_column,\n",
    "            'class*': class_column,\n",
    "            'equipmentType*': equipment_type_column,\n",
    "            'criticality': criticality_column,\n",
    "            'asset name': asset_name_column,\n",
    "            'namespace*': [namespace] * len(barcode_column),\n",
    "            'isActive*': [True] * len(barcode_column)\n",
    "        })\n",
    "\n",
    "        # Align with template columns\n",
    "        template_columns = csv_data_cleaned.columns\n",
    "        for column in template_columns:\n",
    "            if column not in new_cleaned_data_normalized.columns:\n",
    "                new_cleaned_data_normalized[column] = None\n",
    "\n",
    "        new_cleaned_data_normalized = new_cleaned_data_normalized[template_columns]\n",
    "\n",
    "        # Concatenate with cleaned CSV data\n",
    "        updated_csv_cleaned_data_final = pd.concat([csv_data_cleaned, new_cleaned_data_normalized], ignore_index=True)\n",
    "\n",
    "        # Save to the specified output path\n",
    "        updated_csv_cleaned_data_final.to_csv(output_path, index=False)\n",
    "\n",
    "        return f\"Cleaned data has been successfully saved to: {output_path}\"\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "# Create a Gradio interface\n",
    "interface = gr.Interface(\n",
    "    fn=process_asset_equipment_data,\n",
    "    inputs=[\n",
    "        gr.File(label=\"Upload Asset Location File (.xlsx)\"),\n",
    "        gr.File(label=\"Upload CSV File (.csv)\"),\n",
    "        gr.Textbox(label=\"Namespace*\", placeholder=\"Enter namespace here\"),\n",
    "        gr.Textbox(label=\"Output Path\", placeholder=\"Enter the full output file path\"),\n",
    "    ],\n",
    "    outputs=\"text\",\n",
    "    title=\"Asset Equipment Data Processor\",\n",
    "    description=(\n",
    "        \"Upload your Asset Location and Asset Equipment CSV files, provide a namespace, \"\n",
    "        \"specify the output file path, and process the data. The cleaned data will be saved to the specified path.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asset Id's Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7866\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7866/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gradio as gr\n",
    "\n",
    "# Function to process the data\n",
    "def process_space_and_location_data(location_file, space_file, output_path):\n",
    "    try:\n",
    "        # Load the uploaded files\n",
    "        location_data = pd.read_csv(location_file.name)\n",
    "        space_data = pd.read_csv(space_file.name)\n",
    "\n",
    "        # Clean the relevant columns\n",
    "        location_data['name_cleaned'] = location_data['name*'].str.strip()\n",
    "        space_data['asset_name_cleaned'] = space_data['asset name'].astype(str).str.strip()\n",
    "\n",
    "        # Create a mapping dictionary using the location file\n",
    "        location_mapping = location_data.set_index('name_cleaned')['id'].to_dict()\n",
    "\n",
    "        # Populate the \"asset*\" column in the space file based on asset name match\n",
    "        space_data['asset*'] = space_data['asset_name_cleaned'].map(location_mapping)\n",
    "\n",
    "        # Drop the helper column to keep the template unchanged\n",
    "        space_data.drop(columns=['asset_name_cleaned'], inplace=True)\n",
    "\n",
    "        # Save the updated space file to the specified output path\n",
    "        space_data.to_csv(output_path, index=False)\n",
    "\n",
    "        return f\"Data processed successfully! Updated file saved at: {output_path}\"\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "# Create a Gradio interface\n",
    "interface = gr.Interface(\n",
    "    fn=process_space_and_location_data,\n",
    "    inputs=[\n",
    "        gr.File(label=\"Upload Location File (.csv)\"),\n",
    "        gr.File(label=\"Upload Space File (.csv)\"),\n",
    "        gr.Textbox(label=\"Output Path\", placeholder=\"Enter the full output file path\"),\n",
    "    ],\n",
    "    outputs=\"text\",\n",
    "    title=\"Space and Location Data Mapper\",\n",
    "    description=(\n",
    "        \"Upload your Location and Space CSV files, specify the output file path, \"\n",
    "        \"and process the data. The updated file will overwrite the specified Space file.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Barcode Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\remen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The updated file with the 'Barcode' column has been saved to: A:\\Alfuttaim Onboardings\\Doha Onboarding\\Updated_Asset_Barcodes6.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = 'A:\\\\Alfuttaim Onboardings\\\\Doha Onboarding\\\\Copy of Contract and asset data template Issue- IKEA.xlsx'\n",
    "output_file_path = 'A:\\\\Alfuttaim Onboardings\\\\Doha Onboarding\\\\Updated_Asset_Barcodes6.xlsx'\n",
    "\n",
    "# Load the 'Asset,location' sheet\n",
    "excel_data = pd.ExcelFile(file_path)\n",
    "asset_location_df = excel_data.parse('Asset,location')\n",
    "\n",
    "# Set 'Building' column to 'IKEA' for all rows\n",
    "asset_location_df['Building']\n",
    "\n",
    "# Fill missing values in other relevant columns with placeholders\n",
    "asset_location_df['Floor'] = asset_location_df['Floor'].fillna('UnknownFloor')\n",
    "asset_location_df['Sublocation'] = asset_location_df['Sublocation'].fillna('UnknownSublocation')\n",
    "asset_location_df['Asset / Equipment'] = asset_location_df['Asset / Equipment'].fillna('UnknownAsset')\n",
    "\n",
    "# Identify duplicates and add a counter only for duplicate combinations of Facility, Floor, Sublocation, and Asset / Equipment\n",
    "duplicates_mask = asset_location_df.duplicated(subset=['Building', 'Floor', 'Sublocation', 'Asset / Equipment'], keep=False)\n",
    "asset_location_df['Counter'] = 0\n",
    "asset_location_df.loc[duplicates_mask, 'Counter'] = asset_location_df.groupby([\n",
    "    'Building', 'Floor', 'Sublocation', 'Asset / Equipment'\n",
    "]).cumcount() + 1\n",
    "\n",
    "# Create the Barcode column with numbering only for duplicates\n",
    "asset_location_df['Barcode'] = (\n",
    "    asset_location_df['Building'] + \"_\" +\n",
    "    asset_location_df['Floor'].str.replace(' ', '_') + \"_\" +\n",
    "    asset_location_df['Sublocation'].str.replace(' ', '_') + \"_\" +\n",
    "    asset_location_df['Asset / Equipment'].str.replace(' ', '_') +\n",
    "    asset_location_df['Counter'].apply(lambda x: f\"_{x}\" if x > 0 else \"\")\n",
    ")\n",
    "\n",
    "# Drop the Counter column as it's no longer needed\n",
    "asset_location_df = asset_location_df.drop(columns=['Counter'])\n",
    "\n",
    "# Save the updated data to a new Excel file\n",
    "asset_location_df.to_excel(output_file_path, sheet_name='Asset,location', index=False)\n",
    "\n",
    "print(f\"The updated file with the 'Barcode' column has been saved to: {output_file_path}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
